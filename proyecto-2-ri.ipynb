{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":438431,"sourceType":"datasetVersion","datasetId":1939}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-04T04:19:44.861920Z","iopub.execute_input":"2026-02-04T04:19:44.862331Z","iopub.status.idle":"2026-02-04T04:19:44.868709Z","shell.execute_reply.started":"2026-02-04T04:19:44.862302Z","shell.execute_reply":"2026-02-04T04:19:44.867799Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/consumer-reviews-of-amazon-products/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv\n/kaggle/input/consumer-reviews-of-amazon-products/1429_1.csv\n/kaggle/input/consumer-reviews-of-amazon-products/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"!pip install -q -U sentence-transformers faiss-cpu gradio google-generativeai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T04:19:44.870419Z","iopub.execute_input":"2026-02-04T04:19:44.870690Z","iopub.status.idle":"2026-02-04T04:19:49.432986Z","shell.execute_reply.started":"2026-02-04T04:19:44.870659Z","shell.execute_reply":"2026-02-04T04:19:49.432127Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, CrossEncoder, util\nfrom PIL import Image\nimport os\n\nprint(\"--- INICIANDO PARTE A: INDEXACI√ìN (CORREGIDA) ---\")\n\n# 1. Configuraci√≥n de Hardware\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# 2. Carga del Dataset\nruta_kaggle = '/kaggle/input/consumer-reviews-of-amazon-products/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv'\n\ntry:\n    df_raw = pd.read_csv(ruta_kaggle, low_memory=False)\nexcept FileNotFoundError:\n    df_raw = pd.read_csv('Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv', low_memory=False)\n\n# 3. Limpieza de Datos e Im√°genes\ndef limpiar_url(url_str):\n    if pd.isna(url_str): return \"https://via.placeholder.com/150?text=Sin+Imagen\"\n    return url_str.split(',')[0]\n\ndf_raw['imagen_final'] = df_raw['imageURLs'].apply(limpiar_url)\n\n# === CORRECCI√ìN ANTI-DUPLICADOS ===\n# Detectamos cu√°l columna existe y la seleccionamos ESPEC√çFICAMENTE\ncol_origen = 'primaryCategories' if 'primaryCategories' in df_raw.columns else 'categories'\n\n# Creamos el dataframe limpio SELECCIONANDO primero (evita duplicados al renombrar)\ndf = df_raw[['name', col_origen, 'reviews.text', 'imagen_final']].copy()\n\n# Ahora renombramos la columna seleccionada a un est√°ndar 'categories'\ndf.rename(columns={col_origen: 'categories'}, inplace=True)\n\n# Limpieza de nulos\ndf = df.dropna(subset=['name', 'categories'])\n\n# Mezclamos y tomamos 5000 muestras (Shuffle)\n#if len(df) > 5000:\n    #df = df.sample(n=5000, random_state=42).reset_index(drop=True)\n#else:\ndf = df.reset_index(drop=True)\n\nprint(f\"‚úÖ Datos cargados y limpios: {len(df)} productos.\")\n\n# 4. Carga del Modelo Multimodal\nprint(\"‚è≥ Cargando modelo CLIP (puede tardar un poco)...\")\nbi_encoder = SentenceTransformer('clip-ViT-B-32', device=device)\n\n# 5. Generaci√≥n de Embeddings\n# Ahora esto funcionar√° porque 'categories' es una columna √∫nica\ncorpus_textos = (df['name'] + \" \" + df['categories']).tolist()\nprint(\"‚è≥ Generando vectores...\")\ncorpus_embeddings = bi_encoder.encode(corpus_textos, convert_to_tensor=True, show_progress_bar=True)\n\nprint(\"‚úÖ PARTE A COMPLETADA: √çndice vectorial listo.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T04:19:49.434600Z","iopub.execute_input":"2026-02-04T04:19:49.435218Z","iopub.status.idle":"2026-02-04T04:20:11.266317Z","shell.execute_reply.started":"2026-02-04T04:19:49.435183Z","shell.execute_reply":"2026-02-04T04:20:11.265442Z"}},"outputs":[{"name":"stdout","text":"--- INICIANDO PARTE A: INDEXACI√ìN (CORREGIDA) ---\n‚úÖ Datos cargados y limpios: 28332 productos.\n‚è≥ Cargando modelo CLIP (puede tardar un poco)...\n‚è≥ Generando vectores...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/886 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f34204e8cf6e4b3e8e466074ca079cdf"}},"metadata":{}},{"name":"stdout","text":"‚úÖ PARTE A COMPLETADA: √çndice vectorial listo.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"print(\"--- CONFIGURANDO PARTE B: RETRIEVAL ---\")\n\ndef etapa_retrieval_flexible(query_input, input_type=\"text\", k=50):\n    \"\"\"\n    Recupera candidatos iniciales.\n    input_type puede ser: \n    - 'text': Consulta normal\n    - 'image': Ruta de archivo de imagen (image-to-product)\n    - 'vector': Vector matem√°tico (para refinamientos de memoria)\n    \"\"\"\n    query_emb = None\n\n    # 1. Convertir la entrada a vector seg√∫n su tipo\n    if input_type == 'vector':\n        query_emb = query_input\n    elif input_type == 'image':\n        # Procesa la imagen con CLIP\n        try:\n            query_emb = bi_encoder.encode(Image.open(query_input), convert_to_tensor=True)\n        except Exception as e:\n            print(f\"Error al abrir imagen: {e}\")\n            return [], None\n    else: # text\n        query_emb = bi_encoder.encode(query_input, convert_to_tensor=True)\n\n    # 2. B√∫squeda de Similitud (Coseno)\n    # Compara el vector de la consulta contra todo el corpus\n    scores = util.cos_sim(query_emb, corpus_embeddings)[0]\n    \n    # 3. Recuperar Top-K\n    top_results = torch.topk(scores, k=k)\n    \n    return top_results.indices.cpu().numpy(), query_emb\n\nprint(\" PARTE B LISTA: Funci√≥n de b√∫squeda configurada.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T04:20:11.267744Z","iopub.execute_input":"2026-02-04T04:20:11.268034Z","iopub.status.idle":"2026-02-04T04:20:11.275093Z","shell.execute_reply.started":"2026-02-04T04:20:11.268007Z","shell.execute_reply":"2026-02-04T04:20:11.274067Z"}},"outputs":[{"name":"stdout","text":"--- CONFIGURANDO PARTE B: RETRIEVAL ---\n PARTE B LISTA: Funci√≥n de b√∫squeda configurada.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"print(\"--- CONFIGURANDO PARTE C: RE-RANKING ---\")\n\n# Cargamos un modelo especializado en comparar pares (Pregunta <-> Documento)\ncross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device=device)\n\ndef etapa_reranking_cross_encoder(query_text, indices_recuperados):\n    \"\"\"\n    Reordena los candidatos bas√°ndose en una lectura profunda del texto.\n    \"\"\"\n    if len(indices_recuperados) == 0:\n        return pd.DataFrame()\n\n    # Extraer textos de los candidatos\n    candidatos_texto = df.iloc[indices_recuperados]['name'].tolist()\n    \n    # Crear pares para el modelo\n    model_inputs = [[query_text, cand] for cand in candidatos_texto]\n    \n    # Predecir relevancia\n    scores_rerank = cross_encoder.predict(model_inputs)\n    \n    # Ordenar √≠ndices basados en el nuevo score\n    orden_indices = np.argsort(scores_rerank)[::-1] # Descendente\n    \n    # Seleccionar Top 5\n    top_5_indices_locales = orden_indices[:5]\n    indices_finales = [indices_recuperados[i] for i in top_5_indices_locales]\n    \n    return df.iloc[indices_finales].copy()\n\nprint(\" PARTE C LISTA: Cross-Encoder cargado.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T04:20:11.277444Z","iopub.execute_input":"2026-02-04T04:20:11.277908Z","iopub.status.idle":"2026-02-04T04:20:11.839322Z","shell.execute_reply.started":"2026-02-04T04:20:11.277884Z","shell.execute_reply":"2026-02-04T04:20:11.838499Z"}},"outputs":[{"name":"stdout","text":"--- CONFIGURANDO PARTE C: RE-RANKING ---\n PARTE C LISTA: Cross-Encoder cargado.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"print(\"--- CONFIGURANDO PARTE E: MEMORIA ---\")\n\nclass MemoriaSesion:\n    def __init__(self):\n        self.ancla_vector = None # Aqu√≠ guardamos la \"intenci√≥n\" principal (ej. la imagen)\n\n    def actualizar_ancla(self, nuevo_vector):\n        self.ancla_vector = nuevo_vector\n\n    def obtener_vector_combinado(self, vector_texto_refinamiento):\n        \"\"\"\n        Combina el contexto anterior (ancla) con el nuevo texto.\n        F√≥rmula: 70% Ancla (Contexto visual/previo) + 30% Refinamiento (Texto nuevo)\n        \"\"\"\n        if self.ancla_vector is None:\n            return vector_texto_refinamiento\n        \n        # Promedio ponderado para mover el vector hacia la nueva direcci√≥n sin perder el origen\n        vector_fusionado = (self.ancla_vector * 0.7) + (vector_texto_refinamiento * 0.3)\n        return vector_fusionado\n\n# Instanciamos la memoria global\nsesion = MemoriaSesion()\nprint(\"PARTE E LISTA: Sistema de memoria activado.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T04:20:11.840498Z","iopub.execute_input":"2026-02-04T04:20:11.840780Z","iopub.status.idle":"2026-02-04T04:20:11.847647Z","shell.execute_reply.started":"2026-02-04T04:20:11.840755Z","shell.execute_reply":"2026-02-04T04:20:11.846706Z"}},"outputs":[{"name":"stdout","text":"--- CONFIGURANDO PARTE E: MEMORIA ---\nPARTE E LISTA: Sistema de memoria activado.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import google.generativeai as genai\nimport time\n\nprint(\"--- CONFIGURANDO PARTE D: RAG (INTENTO GEMINI 2.5 CON RESPALDO) ---\")\n\nMY_API_KEY = \"AIzaSyAazs8UMCaxaRDQBw7cA-owoCyCk5V8qfU\"\n\ngenai.configure(api_key=MY_API_KEY)\n\n# -----------------------------------------------------------\n# SELECTOR DE MODELO INTELIGENTE\n# Intenta cargar el 2.5/2.0 Flash primero. Si falla, usa el Pro.\n# -----------------------------------------------------------\nmodelos_a_probar = ['gemini-2.5-flash', 'gemini-2.0-flash', 'gemini-1.5-flash', 'gemini-pro']\ngemini = None\nnombre_modelo_final = \"\"\n\nfor modelo in modelos_a_probar:\n    try:\n        print(f\"üõ†Ô∏è Probando modelo: {modelo}...\")\n        temp_model = genai.GenerativeModel(modelo)\n        # Hacemos una prueba muda para ver si responde o da error 404/429\n        temp_model.generate_content(\"test\", request_options={'timeout': 5})\n        gemini = temp_model\n        nombre_modelo_final = modelo\n        print(f\"‚úÖ ¬°√âXITO! Usando: {modelo}\")\n        break\n    except Exception as e:\n        print(f\"‚ùå {modelo} fall√≥ o no disponible. Error: {str(e)[:50]}...\")\n        continue\n\nif gemini is None:\n    print(\"‚ö†Ô∏è Fallaron todos los modelos Flash. Usando 'gemini-pro' como √∫ltimo recurso.\")\n    gemini = genai.GenerativeModel('gemini-pro')\n    nombre_modelo_final = \"gemini-pro\"\n\n# -----------------------------------------------------------\n# FUNCI√ìN GENERATIVA\n# -----------------------------------------------------------\ndef generar_respuesta_rag(consulta_usuario, df_resultados):\n    # 1. Construcci√≥n del Contexto\n    contexto = \"\"\n    for _, row in df_resultados.iterrows():\n        contexto += f\"- Producto: {row['name']}\\n  Categor√≠a: {row['categories']}\\n  Rese√±a: {row['reviews.text'][:300]}...\\n\\n\"\n\n   # Pront\n    prompt = f\"\"\"\n    Eres un asistente experto de la EPN. \n    El sistema ha realizado una b√∫squeda (posiblemente visual o por texto) y ha recuperado estos productos:\n    {contexto}\n    \n    Pregunta/Acci√≥n del usuario: \"{consulta_usuario}\"\n    \n    Instrucciones:\n    1. Si el usuario subi√≥ una imagen, describe los productos recuperados que m√°s se parecen a ella.\n    2. Usa las rese√±as para decir si son buenos productos o no.\n    3. No digas que 'no hay informaci√≥n sobre b√∫squeda por imagen', simplemente act√∫a como un vendedor que ve lo que el usuario busca.\"\"\"\n    \n    # 3. Llamada a la API\n    try:\n        # Si usamos un modelo Flash, podemos ser m√°s r√°pidos (menos sleep)\n        tiempo_espera = 1 if \"flash\" in nombre_modelo_final else 3\n        time.sleep(tiempo_espera) \n        \n        response = gemini.generate_content(prompt)\n        return response.text\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Alerta API: {e}\")\n        return f\"Nota: Hubo una interrupci√≥n moment√°nea con el modelo {nombre_modelo_final}, pero aqu√≠ tienes los productos encontrados (ver cat√°logo visual).\"\n\nprint(f\"‚úÖ PARTE D LISTA (Modelo activo: {nombre_modelo_final})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T04:20:11.848678Z","iopub.execute_input":"2026-02-04T04:20:11.849125Z","iopub.status.idle":"2026-02-04T04:20:12.825492Z","shell.execute_reply.started":"2026-02-04T04:20:11.849099Z","shell.execute_reply":"2026-02-04T04:20:12.824605Z"}},"outputs":[{"name":"stdout","text":"--- CONFIGURANDO PARTE D: RAG (INTENTO GEMINI 2.5 CON RESPALDO) ---\nüõ†Ô∏è Probando modelo: gemini-2.5-flash...\n‚úÖ ¬°√âXITO! Usando: gemini-2.5-flash\n‚úÖ PARTE D LISTA (Modelo activo: gemini-2.5-flash)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import gradio as gr\n\nprint(\"--- CONFIGURANDO PARTE F: INTERFAZ (CON ARREGLO DE IM√ÅGENES) ---\")\n\ndef pipeline_principal(texto, imagen, es_refinamiento):\n    global sesion\n    \n    indices = []\n    vector_busqueda = None\n    query_rerank = \"\" \n    texto_usuario = \"\"\n\n    # --- L√ìGICA DE B√öSQUEDA (Igual que antes) ---\n    if imagen is not None:\n        indices, vector_busqueda = etapa_retrieval_flexible(imagen, input_type='image')\n        sesion.actualizar_ancla(vector_busqueda)\n        query_rerank = \"Visualmente similar\"\n        texto_usuario = \"B√∫squeda por imagen\"\n    elif texto:\n        vector_texto = bi_encoder.encode(texto, convert_to_tensor=True)\n        texto_usuario = texto\n        if es_refinamiento and sesion.ancla_vector is not None:\n            vector_busqueda = sesion.obtener_vector_combinado(vector_texto)\n            indices, _ = etapa_retrieval_flexible(vector_busqueda, input_type='vector')\n            sesion.actualizar_ancla(vector_busqueda)\n            query_rerank = texto\n        else:\n            vector_busqueda = vector_texto\n            indices, _ = etapa_retrieval_flexible(texto, input_type='text')\n            sesion.actualizar_ancla(vector_busqueda)\n            query_rerank = texto\n    else:\n        return \"Escribe algo...\", \"\"\n\n    # Re-ranking\n    top_productos = etapa_reranking_cross_encoder(query_rerank, indices)\n    if top_productos.empty: return \"No se encontraron productos.\", \"\"\n\n    # Generaci√≥n RAG (Con el blindaje que pusimos en Parte D)\n    respuesta_ia = generar_respuesta_rag(texto_usuario, top_productos)\n    \n    # --- GALER√çA VISUAL MEJORADA ---\n    galeria_html = \"<div style='display: flex; flex-wrap: wrap; gap: 15px; justify-content: center;'>\"\n    \n    for _, row in top_productos.iterrows():\n        # URL de respaldo por si falla la original\n        img_backup = \"https://via.placeholder.com/150?text=Sin+Imagen\"\n        \n        tarjeta = f\"\"\"\n        <div style=\"border: 1px solid #e0e0e0; padding: 10px; width: 180px; border-radius: 8px; background: white; box-shadow: 0 2px 5px rgba(0,0,0,0.1);\">\n            <div style=\"height: 150px; display: flex; align-items: center; justify-content: center; overflow: hidden;\">\n                <img src=\"{row['imagen_final']}\" \n                     onerror=\"this.onerror=null; this.src='{img_backup}';\" \n                     style=\"max-height: 100%; max-width: 100%; object-fit: contain;\">\n            </div>\n            <h4 style=\"margin: 8px 0; font-size: 13px; color: #333; height: 3.6em; overflow: hidden; display: -webkit-box; -webkit-line-clamp: 3; -webkit-box-orient: vertical;\">\n                {row['name']}\n            </h4>\n            <span style=\"font-size: 10px; color: #666; background: #f0f0f0; padding: 2px 5px; border-radius: 4px;\">\n                {str(row['categories'])[:20]}\n            </span>\n        </div>\n        \"\"\"\n        galeria_html += tarjeta\n        \n    galeria_html += \"</div>\"\n    \n    return respuesta_ia, galeria_html\n\n# Lanzamiento\nwith gr.Interface(\n    fn=pipeline_principal,\n    inputs=[\n        gr.Textbox(label=\"üîç ¬øQu√© buscas?\", placeholder=\"Ej: Tablet for kids...\"),\n        gr.Image(type=\"filepath\", label=\"üì∑ B√∫squeda Visual\"),\n        gr.Checkbox(label=\"üîó Refinar (Memoria)\", value=False)\n    ],\n    outputs=[\n        gr.Markdown(label=\"ü§ñ Asistente\"),\n        gr.HTML(label=\"üõçÔ∏è Cat√°logo\")\n    ],\n    title=\"üõí Buscador Multimodal Amazon\",\n    description=\"Sistema RAG H√≠brido: Texto + Imagen + Memoria.\"\n) as demo:\n    demo.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T04:20:12.826792Z","iopub.execute_input":"2026-02-04T04:20:12.827051Z","iopub.status.idle":"2026-02-04T04:20:14.456511Z","shell.execute_reply.started":"2026-02-04T04:20:12.827025Z","shell.execute_reply":"2026-02-04T04:20:14.455407Z"}},"outputs":[{"name":"stdout","text":"--- CONFIGURANDO PARTE F: INTERFAZ (CON ARREGLO DE IM√ÅGENES) ---\n* Running on local URL:  http://127.0.0.1:7862\n* Running on public URL: https://4cbe13025387a54188.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://4cbe13025387a54188.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"‚ö†Ô∏è Alerta API: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 25.842748128s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 20\n}\n, retry_delay {\n  seconds: 25\n}\n]\n‚ö†Ô∏è Alerta API: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 54.693924488s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 20\n}\n, retry_delay {\n  seconds: 54\n}\n]\n‚ö†Ô∏è Alerta API: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\nPlease retry in 34.01977226s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.5-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 20\n}\n, retry_delay {\n  seconds: 34\n}\n]\n","output_type":"stream"}],"execution_count":27}]}